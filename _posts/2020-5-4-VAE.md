---
layout: post
title:  "无监督学习：AE到VAE"
categories: Unsupervised-Learning
tags:  Unsuperciesed-Learning
excerpt: 记录无监督学习的基本模型:AE以及VAE
author: wjiang
mathjax: true
---

* content
{:toc}

## 参考链接

本人的学习顺序
* [李宏毅机器学习视频](https://www.bilibili.com/video/BV1JE411g7XF?t=2024)

* [变分自编码器（一）：原来是这么一回事](https://kexue.fm/archives/5253)

* [Tutorial - What is a variational autoencoder?](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)

下文的大部分图片及资料均来自上述链接。

## 引言
无监督学习是从无标注的数据中学习数据的统计规律或者说内在结构的机器学习，主要包括聚类、降维、概率估计。无监督学习可以用于数据分析或者监督学习的前处理。


无监督学习使用无标注数据$U = {x_1, x_2, ..., x_N}$学习或训练，无监督学习的模型是函数$z = g_{\theta}(z)$，条件概率分布$P_{\theta}(z\|x)$，或条件概率分布$P_{\theta}(x\|z)$。其中$x \in X$是输入，表示样本；$z \in Z$是输出，表示对样本的分析结果，可以是类别、转换、概率；$\theta$是参数。
(摘自统计学习方法)

## Deep Auto-Encoder(AE)
Deep AE是一种用神经网络对数据进行压缩的模型。简单来讲就是我们希望有一个模型(encoder)能将原始的数据（向量）$x$压缩成一个维度小的多的向量$z$，$z$中包含了$x$尽可能完整的重要信息。那么如何得到比较好的$z$或者说我们如何训练这个encoder？我们构造另一个模型(decoder)，输入是$z$，输出为$\hat{x}$，若能从$z$尽可能完整的还原$x$也就是$\hat{x}$和$x$越像，则说明$z$越好。所以AE的模型框架如下。

![AE](/src/2020-5-4-VAE/AE.png)

图中的code就是压缩后的信息，其维度要比$x$的维度小得多（如果维度大于等于$x$的维度，那么理论上只要做一个copy操作到code中，其他维度都没有用；维度小就会迫使网络去提取$x$中有用的信息）。code的左侧就是encoder，右侧就是decoder，训练时的reconstruction loss就是$Loss(x, \hat{x})$，例如图片可以直接用MSE损失。


AE有着许许多多应用，不在此赘述。BERT可以看成是一个特殊的AE（De-noising auto-encoder）。输入为一个句子，然后对句子增加一些噪音（mask掉一些词），我们希望模型能尽可能的还原出来原始的句子。

## Variational Auto-Encoder(VAE)
从名字可以看出，VAE是AE的变种，所以它大体上仍然遵循AE的框架，通过encoder得到输入$x$的隐变量$z$，再通过decoder将$z$还原成$x$, 如下图：

![VAE](/src/2020-5-4-VAE/VAE1.jpg)

但是具体模型上有两个不同，如下图所示：

![VAE](/src/2020-5-4-VAE/VAE2.jpg)

第一个是隐变量$z$。在VAE中，encoder输出两个向量$\sigma$和$m$，然后从标准正态分布中采样一个向量$e$，通过公式$c = exp(\sigma) * e + m$(下文c统一写作z)，再将$c$输入到decoder中。
第二个是loss function。在VAE中，loss为

$$l(\theta, \phi) = -E_{z \sim q_{\theta}(z|x)}[log p_{\phi}(x|z)] + KL(q_{\theta}(z|x)||p(z))$$

其中第一项就对应了reconstruction error或者说expected negative log-likelihood，本质上和AE中的loss一样。具体来讲是从encoder输出的distribution即$q_{\theta}(z|x)$中sample出一个$z$(图2中encoder做的事情)，根据这个$z$ decoder会得到一个distribution即$p_{\phi}(x\|z)$,我们希望原始数据$x$在该分布中的概率越大越好，这样我们就越有可能sample(重构)出$x$。
而第二项展开来对应了图2中的下面公式。KL距离是衡量两个概率分布的相似程度的函数，一般情况下，我们设定$p(z)$是标准正态分布，$q_{\theta}(z\|x)$是一个正态分布，参数由神经网络给出(理论上可以用其它分布表示)。推导见后文。


那么为什么VAE这样设计的原理是什么？或者说为什么要和AE有这样的区别？