---
layout: post
title:  "无监督学习：AE到VAE"
categories: Unsupervised-Learning
tags:  Unsuperciesed-Learning
excerpt: 记录无监督学习的基本模型:AE以及VAE
author: wjiang
mathjax: true
---

* content
{:toc}

## 参考链接

本人的学习顺序
* [https://www.bilibili.com/video/BV1JE411g7XF?t=2024](李宏毅机器学习视频)

* [https://kexue.fm/archives/5253](变分自编码器（一）：原来是这么一回事)

* [https://jaan.io/what-is-variational-autoencoder-vae-tutorial/](Tutorial - What is a variational autoencoder?)

下文的大部分图片及资料均来自上述链接。

## 引言
无监督学习是从无标注的数据中学习数据的统计规律或者说内在结构的机器学习，主要包括聚类、降维、概率估计。无监督学习可以用于数据分析或者监督学习的前处理。


无监督学习使用无标注数据$U = {x_1, x_2, ..., x_N}$学习或训练，无监督学习的模型是函数$z = g_{\theta}(z)$，条件概率分布$P_{\theta}(z\|x)$，或条件概率分布$P_{\theta}(x\|z)$。其中$x \in X$是输入，表示样本；$z \in Z$是输出，表示对样本的分析结果，可以是类别、转换、概率；$\theta$是参数。
(统计学习方法)

## Deep AutoEncoder-AE
Deep AE是一种用神经网络对数据进行压缩的模型。简单来讲就是我们希望有一个模型(encoder)能将原始的数据（向量）$x$压缩成一个维度小的多的变量$z$，$z$中包含了$x$尽可能完整的重要的信息。那么如何得到比较好的$z$或者说我们如何训练这个encoder？我们构造另一个模型(decoder)，输入是$z$，输出为$\hat{x}$，若能从$z$尽可能完整的还原$x$也就是$\hat{x}$和$x$越像，则说明$z$越好。所以AE的模型框架如下。

![AE](/src/2020-5-4-VAE/AE.png)

图中的code就是压缩后的信息，其维度要比$x$的维度小得多（如果维度大于等于$x$的维度，那么理论上只要做一个copy操作到code中，其他维度都没有用；维度小就会迫使网络去提取$x$中有用的信息）。code的左侧就是encoder，右侧就是decoder，训练时的loss就是$Loss(x, \hat{x})$，例如图片可以直接用MSE损失。

