---
layout: post
title:  "无监督学习：AE到VAE"
categories: Unsupervised-Learning
tags:  Unsuperciesed-Learning
excerpt: 记录无监督学习的基本模型:AE以及VAE
author: wjiang
mathjax: true
---

* content
{:toc}

## 参考链接

本人的学习顺序
* [李宏毅机器学习视频](https://www.bilibili.com/video/BV1JE411g7XF?t=2024)

* [变分自编码器（一）：原来是这么一回事](https://kexue.fm/archives/5253)

* [Tutorial - What is a variational autoencoder?](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)

下文的大部分图片及资料均来自上述链接。

## 引言
无监督学习是从无标注的数据中学习数据的统计规律或者说内在结构的机器学习，主要包括聚类、降维、概率估计。无监督学习可以用于数据分析或者监督学习的前处理。


无监督学习使用无标注数据$U = {x_1, x_2, ..., x_N}$学习或训练，无监督学习的模型是函数$z = g_{\theta}(z)$，条件概率分布$P_{\theta}(z\|x)$，或条件概率分布$P_{\theta}(x\|z)$。其中$x \in X$是输入，表示样本；$z \in Z$是输出，表示对样本的分析结果，可以是类别、转换、概率；$\theta$是参数。
(摘自统计学习方法)

## Deep Auto-Encoder(AE)
Deep AE是一种用神经网络对数据进行压缩的模型。简单来讲就是我们希望有一个模型(encoder)能将原始的数据（向量）$x$压缩成一个维度小的多的向量$z$，$z$中包含了$x$尽可能完整的重要信息。那么如何得到比较好的$z$或者说我们如何训练这个encoder？我们构造另一个模型(decoder)，输入是$z$，输出为$\hat{x}$，若能从$z$尽可能完整的还原$x$也就是$\hat{x}$和$x$越像，则说明$z$越好。所以AE的模型框架如下。

![AE](/src/2020-5-4-VAE/AE.png)

图中的code就是压缩后的信息，其维度要比$x$的维度小得多（如果维度大于等于$x$的维度，那么理论上只要做一个copy操作到code中，其他维度都没有用；维度小就会迫使网络去提取$x$中有用的信息）。code的左侧就是encoder，右侧就是decoder，训练时的reconstruction loss就是$Loss(x, \hat{x})$。


AE有着许许多多应用，不在此赘述。BERT可以看成是一个特殊的AE（De-noising auto-encoder）。输入为一个句子，然后对句子增加一些噪音（mask掉一些词），我们希望模型能尽可能的还原出来原始的句子。

## Variational Auto-Encoder(VAE)
从名字可以看出，VAE是AE的变种，所以它大体上仍然遵循AE的框架，通过encoder得到输入$x$的隐变量$z$，再通过decoder将$z$还原成$x$, 如下图：

![VAE](/src/2020-5-4-VAE/VAE1.jpg)

但是具体模型上有两个不同，如下图所示：

![VAE](/src/2020-5-4-VAE/VAE2.jpg)

第一个是隐变量$z$。在VAE中，encoder输出两个向量$\sigma$和$m$，然后从标准正态分布中采样一个向量$e$，通过公式$c = exp(\sigma) * e + m$($c$相当于隐变量$z$)，再将$c$输入到decoder中。
第二个是loss function。在VAE中，loss为

$$l(\theta, \phi) = -E_{z \sim q_{\theta}(z|x)}[log p_{\phi}(x|z)] + KL(q_{\theta}(z|x)||p(z))$$

其中第一项就对应了reconstruction error或者说expected negative log-likelihood，本质上和AE中的loss一样。具体来讲是从encoder输出的distribution即$q_{\theta}(z|x)$中sample出一个$z$(图2中encoder做的事情)，根据这个$z$ decoder会得到一个distribution即$p_{\phi}(x\|z)$,我们希望原始数据$x$在该分布中的概率越大越好，这样我们就越有可能sample(重构)出$x$。
而第二项展开来对应了图2中的下面公式。KL距离是衡量两个概率分布的相似程度的函数，一般情况下，我们设定$p(z)$是标准正态分布，$q_{\theta}(z\|x)$是一个正态分布，参数由神经网络给出(理论上可以用其它分布表示)。计算推导见后文。

那么VAE这样设计的原理是什么？或者说为什么要和AE有这样的区别？下面从两个角度解释。

### Why VAE: 从神经网络的角度
从直觉上看，我们可以将encoder输出的$m$看作是最初的隐变量，加上的部分$exp(\sigma) * e$看作是噪声。这个噪声的分布是
$N(0, exp^{2}(\sigma)$(利用性质：如果$X \sim N(\mu, \sigma^{2})$，那么$aX+b \sim N(a\mu+b, (a\sigma)^2)$，下文还要用到)。如果仅仅加上了噪声会出现问题。噪声增加了重构的难度，所以模型会趋向于让噪声的方差为0，这样噪声就失去了随机性(如果正态分布的方差为0，则仅在均值的地方概率为1，其它为0)，也就失去了噪声的意义。说白了，模型会慢慢退化成普通的Auto-Encoder，噪声不再起作用。

所以在loss中增加了一项KL距离，希望$q_{\theta}(z\|x)$向$p(z)$也就是标准正态分布看齐，这样我们就可以从标准正态分布中采样$z$来生成我们想要的东西(传统的AE中，$z$的分布未知，对于相似的图片，它们的隐变量表示可能是截然不同的，没有意义的。所以我们很难控制$z$去做有效的生成)。具体来看展开的loss公式，$m^{2}$希望$m$趋向于0，也就是均值为0，也可以看作是$m$的L2正则项。而$exp(\sigma) - (1 + \sigma)$这个loss仅在$\sigma=0$时有最小值0，所以希望$\sigma$趋向于0，也就是噪声的方差趋向于1。

综上所述，VAE中增加了高斯噪声，使得模型更具有鲁棒性和泛化性；同时在loss中增加了KL距离，使得噪声的方差趋向为1，以及让$z$的均值为0。这样做一是为了让噪声不失效，同时也让我们更容易采样$z$来做生成(参考视频讲解例子)。

![VAE](/src/2020-5-4-VAE/VAE3.jpg)