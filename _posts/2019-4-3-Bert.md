---
layout: post
title:  "BERT的使用记录"
categories: tools
tags:  NLP tools
excerpt: 记录使用BERT模型的方法
author: wjiang
---

* content
{:toc}

## 提要

本文记录一些本人使用BERT模型([论文链接](https://arxiv.org/pdf/1810.04805.pdf))的经验和方法。针对论文中的两种使用方法，第一种是fine-tuning，由于我使用的框架是Pytorch而非TensorFlow，所以我借鉴了github上的[Pytorch-Bert](https://github.com/huggingface/pytorch-pretrained-BERT)。第二种是提取特征方法，类似预训练词向量，把每个词的向量表示保存到磁盘上。

## fine-tuning method

本节结合代码描述如何用预训练的BERT搭建一个基于词的分类模型。需要安装pytorch以及pytorch-bert模块。最好用迅雷单独下载预训练好的模型。

### 准备tokenizer

由于BERT使用的是subword，所以它会把一些词拆成多个字词，导致输入输出不能对齐。根据论文中的描述，如果一个词被拆开，我们取第一个子词的输出表示（应该也能取最后一个，或者取平均）。我们要用到内置的BertTokenizer类，如下面语句所示，主要用来拆解词以及把词转换为索引。

```python
from pytorch_pretrained_bert.tokenization import BertTokenizer
```

还有一个容易出错的地方是BERT会忽略一些空白字符，而往往为了对齐我们需要在该位置添加“[PAD]”字符。官方提供了一些判断的函数，如下：

```python
import unicodedata


def _is_whitespace(char):
    """Checks whether `chars` is a whitespace character."""
    # \t, \n, and \r are technically contorl characters but we treat them
    # as whitespace since they are generally considered as such.
    if char == " " or char == "\t" or char == "\n" or char == "\r":
        return True
    cat = unicodedata.category(char)
    if cat == "Zs":
        return True
    return False


def _is_control(char):
    """Checks whether `chars` is a control character."""
    # These are technically control characters but we count them as whitespace
    # characters.
    if char == "\t" or char == "\n" or char == "\r":
        return False
    cat = unicodedata.category(char)
    if cat.startswith("C"):
        return True
    return False


def _is_punctuation(char):
    """Checks whether `chars` is a punctuation character."""
    cp = ord(char)
    # We treat all non-letter/number ASCII as punctuation.
    # Characters such as "^", "$", and "`" are not in the Unicode
    # Punctuation class but we treat them as punctuation anyways, for
    # consistency.
    if (
        (cp >= 33 and cp <= 47)
        or (cp >= 58 and cp <= 64)
        or (cp >= 91 and cp <= 96)
        or (cp >= 123 and cp <= 126)
    ):
        return True
    cat = unicodedata.category(char)
    if cat.startswith("P"):
        return True
    return False


def _clean_text(text):
    output = []
    for char in text:
        cp = ord(char)
        if cp == 0 or cp == 0xFFFD or _is_control(char):
            continue
        if _is_whitespace(char):
            output.append(" ")
        else:
            output.append(char)
    return "".join(output)


def judge_ignore(word):
    if len(_clean_text(word)) == 0:
        return True
    for char in word:
        cp = ord(char)
        if cp == 0 or cp == 0xFFFD or _is_control(char):
            return True
    return False
```

有了上面的准备，我们可以实现一个简单的vocab类来准备数据：
```python

def flatten(list_of_lists):
    for list in list_of_lists:
        for item in list:
            yield item

class Vocab(object):
    def __init__(self, bert_vocab_path):
        self.tokenizer = BertTokenizer.from_pretrained(
            bert_vocab_path, do_lower_case=False
        )

    def convert_tokens_to_ids(self, tokens):
        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)
        ids = torch.tensor(token_ids, dtype=torch.long)
        mask = torch.ones(len(ids), dtype=torch.long)
        return ids, mask

    def subword_tokenize(self, tokens):
        subwords = list(map(self.tokenizer.tokenize, tokens))
        subword_lengths = [1] + list(map(len, subwords)) + [1]
        subwords = ["[CLS]"] + list(flatten(subwords)) + ["[SEP]"]
        token_start_idxs = torch.cumsum(torch.tensor([0] + subword_lengths[:-1]), dim=0)
        return subwords, token_start_idxs

    def subword_tokenize_to_ids(self, tokens):
        tokens = ["[PAD]" if judge_ignore(t) else t for t in tokens]
        subwords, token_start_idxs = self.subword_tokenize(tokens)
        subword_ids, mask = self.convert_tokens_to_ids(subwords)
        token_starts = torch.zeros(len(subword_ids), dtype=torch.uint8)
        token_starts[token_start_idxs] = 1
        return subword_ids, mask, token_starts

    def tokenize(self, tokens):
        subwords = list(map(self.tokenizer.tokenize, tokens))
        subword_lengths = [1] + list(map(len, subwords)) + [1]
        subwords = ["[CLS]"] + list(flatten(subwords)) + ["[SEP]"]
        return subwords
```

### 创建模型

现在创建我们的tagger模型。只需要内置的BertModel作为我们的encoder以及一个线性层来做标签的分类。它的输入有三个，第一个就是所有字词对应的索引，它的维度应当为[B, L], 第二个是所有子词对应的掩码，第三个是原始词对应的第一个子词的掩码。如下图:

```python
import torch
import torch.nn as nn
from pytorch_pretrained_bert.modeling import BertModel
from torch.nn.utils.rnn import pad_sequence

class Bert_Tagger(nn.Module):
    def __init__(self, bert_path):
        super(Bert_Tagger, self).__init__()
        self.bert = BertModel.from_pretrained(bert_path)
        self.classifier = nn.Linear(768, 10)  # 假设有10个可能的标签

    def forward(self, subword_idxs, subword_masks, token_starts):
        bert_outs, _ = self.bert(
            subword_idxs,
            token_type_ids=None,
            attention_mask=subword_masks,
            output_all_encoded_layers=False,
        )
        lens = token_starts.sum(dim=1)

        bert_outs = torch.split(bert_outs[token_starts], lens.tolist())
        bert_outs = pad_sequence(bert_outs, batch_first=True) # 也可以不pad，一个句子一个句子做
        out = self.classifier(bert_outs)    # B*L*10
        mask = torch.arange(max_len) < lens.unsqueeze(-1) #如果pad了，可能要用到此mask来算loss或者预测

        return out
```

### 测试

我们以输入两个句子为例，batch_size就是2。

```python
if __name__ == "__main__":
    bert_path = "/data/wjiang/data/bert/bert-base-cased.tar.gz"
    bert_vocab_path = "/data/wjiang/data/bert/bert-base-cased-vocab.txt"

    vocab = Vocab(bert_vocab_path)
    examples = [["I", "am", "jiangwei"], ["Hello"]]
    subword_idxs, subword_masks, token_starts_masks = [], [], []
    for e in examples:
        print(vocab.tokenize(e))
        subword_ids, mask, token_starts = vocab.subword_tokenize_to_ids(e)
        token_starts[[0, -1]] = 0   #最后的结果忽略句首句尾
        subword_idxs.append(subword_ids)
        subword_masks.append(mask)
        token_starts_masks.append(token_starts)

    subword_idxs = pad_sequence(subword_idxs, batch_first=True)
    subword_masks = pad_sequence(subword_masks, batch_first=True)
    token_starts_masks = pad_sequence(token_starts_masks, batch_first=True)

    model = Bert_Tagger(bert_path)
    out = model(subword_idxs, subword_masks, token_starts_masks)
    print(out.shape)
```

## extract fixed features

详情见[Bert](https://github.com/google-research/bert)文档，有空再添加。


## 参考

* [BERT Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)

* [Bert](https://github.com/google-research/bert)

* [Pytorch-Bert](https://github.com/huggingface/pytorch-pretrained-BERT)